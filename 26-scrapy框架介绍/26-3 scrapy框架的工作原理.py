"""
普通的爬虫流程:
1 > 确认目标的url
2 > 发送网络请求获取响应数据
3 > 解析数据
4 > 保存
"""

"""
scrapy框架基本五个部分(组件)
中 > 引擎engine
上 > 调度器scheduler
左 > 管道 item pipeline 
下 > 爬虫器(爬虫组件) 
右 > 下载器 downloader
"""

"""
scrapy框架基本五个组件都干了些啥
引擎 > C位，不用做实事，接收任务，分发任务
爬虫器 > 1.确认目标的url,根据这个url会构造一个request对象(以后都会在代码当中有所体现)，交给引擎
        4.接收来自引擎的response对象，进行解析，解析完毕，把结果给引擎
        解析结果分为2种:
            --(1) 如果提取出的是url,就把所有的步骤重新再走一遍
            --(2) 如果是需要进行保存的数据data
        
调度器 > 2.接收了来自引擎的request对象(假设有100个request对象)，安排调度，安排谁先谁后，进行一个排序，把排序之后的交给引擎
下载器 > 3.接收了来自经过了排序之后的引擎的request对象，发送请求，获取响应对象response,交给引擎
管道  > 5.接收来自引擎的数据data,进行保存
"""


"""
以上介绍的部分，哪些是需要我们自己去做，哪些不需要
自己完成:起始的url的确认，数据的提取，保存
"""